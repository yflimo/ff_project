# 《高级人工智能》机器学习核心内容整理
## 一、完整框架结构
### 1. 机器学习概述
1.1 机器学习的定义与任务
- 1.1.1 定义
- 1.1.2 基本任务分类
- 1.1.3 四大学习策略
  - （1）监督学习（Supervised Learning）
  - （2）无监督学习（Unsupervised Learning）
  - （3）半监督学习（Semi-Supervised Learning）
  - （4）强化学习（Reinforcement Learning）
1.2 机器学习的一般流程
- 1.2.1 数据收集与预处理
- 1.2.2 模型选择与训练
- 1.2.3 模型评估与调优
1.3 机器学习模型的评估指标
- 1.3.1 分类任务指标
- 1.3.2 回归任务指标
- 1.3.3 聚类任务指标
- 1.3.4 生成任务指标
- 1.3.5 指标选择原则

### 2. 统计机器学习
2.1 监督学习
- 2.1.1 线性回归（回归任务基准模型）
- 2.1.2 逻辑回归（分类任务常用模型）
- 2.1.3 决策树（可解释性强的非线性模型）
2.2 无监督学习
- 2.2.1 K-Means 聚类
- 2.2.2 主成分分析（PCA）
- 2.2.3 关联规则挖掘（Apriori 算法）
2.3 半监督学习
- 2.3.1 标签传播（Label Propagation, LP）

### 3. 强化学习
3.1 强化学习基础概念
- 3.1.1 定义
- 3.1.2 强化学习的要素
- 3.1.3 强化学习的目标与任务类型
- 3.1.4 强化学习的步骤
- 3.1.5 强化学习的应用场景
3.2 马尔可夫决策过程（MDP）
- 3.2.1 定义
- 3.2.2 MDP 的五要素（S, A, P, R, γ）
- 3.2.3 MDP 的“状态转移与奖励流程”
- 3.2.4 MDP 在强化学习中的作用
- 3.2.5 常见问题与扩展
3.3 强化学习算法：Q 学习
- 3.3.1 定义
- 3.3.2 Q 学习的要素
- 3.3.3 Q 学习的步骤
- 3.3.5 Q 学习的特性
- 3.3.6 常见问题与解决办法
3.4 深度 Q 网络（DQN）
- 3.4.1 定义
- 3.4.2 DQN 对传统 Q 学习的三大改进
- 3.4.3 DQN 的算法流程
- 3.4.5 DQN 的应用场景与变种

## 二、名词解释
### 1. 核心基础概念
#### 机器学习
- 定义：让计算机从数据中自动学习规律，无需显式编程即可完成任务的技术；本质是通过数据驱动优化模型参数，实现对未知数据的预测或决策（泛化能力）。
- 适用场景：各类数据预测、决策、模式挖掘任务，如房价预测、图像识别、客户分群等。

#### 监督学习
- 定义：利用带标签的训练数据（输入→输出的明确映射），让模型学习“输入到标签的映射规则”，最终实现对新数据的标签预测。
- 数据特点：有明确“输入-输出标签对”，标签需人工标注，数据质量要求高。
- 学习逻辑：通过最小化“预测值与真实标签的误差”，反向调整模型参数，拟合映射规则。
- 适用场景：有明确预测目标的场景，如垃圾邮件识别、疾病诊断、房价预测等。

#### 无监督学习
- 定义：仅利用无标签的训练数据，让模型自主挖掘数据的“内在结构或规律”，如聚类、降维，无需人工指定输出目标。
- 数据特点：只有输入数据，无任何标签，无需人工标注。
- 学习逻辑：模型通过分析数据的统计特征（距离、密度、方差），发现数据的自然分组或冗余信息。
- 适用场景：探索数据规律的场景，如客户分群、异常检测、高维数据可视化等。

#### 半监督学习
- 定义：结合少量带标签数据和大量无标签数据训练模型，利用无标签数据的结构信息辅助标签学习，解决“标签稀缺”问题。
- 数据特点：少量标签数据（10%~30%）+ 大量无标签数据（70%~90%），标签标注成本低。
- 学习逻辑：先通过无标签数据挖掘数据结构（如聚类），再用少量标签“指导”模型优化映射规则。
- 适用场景：标签标注成本高的场景，如行业文档分类、医疗影像分类、方言语音识别等。

#### 强化学习
- 定义：智能体（Agent）通过与环境持续交互，接收环境反馈的“奖励/惩罚信号”，逐步学习“最大化累计奖励”的最优策略（动作选择规则）。
- 数据特点：无标签数据，有“环境-动作-奖励”的交互序列，数据通过交互生成。
- 学习逻辑：智能体执行动作→环境返回新状态+奖励→模型更新策略（奖励为正强化、惩罚为负强化），迭代优化。
- 适用场景：序列决策与交互场景，如游戏AI、机器人控制、自动驾驶、资源调度等。

### 2. 核心算法概念
#### 线性回归
- 定义：监督学习中回归任务的基准模型，假设自变量（特征）与因变量（目标值）之间存在线性相关关系，通过学习数据中的规律，构建线性函数来拟合数据，进而实现预测。
- 公式：单变量线性回归 \(y=wx+b\)；多变量线性回归 \(y=w_1x_1+w_2x_2+...+w_nx_n+b\)；损失函数 \(L(w,b)=\frac{1}{n}\sum_{i=1}^n(y_i-(wx_i+b))^2\)。
- 基础前提：自变量与因变量线性相关；观测数据之间相互独立，残差无相关性；残差呈正态分布。
- 适用场景：房价预测、销量预测、气温预测等连续输出的任务。

#### 逻辑回归
- 定义：监督学习二分类任务的基准模型，通过Sigmoid函数将线性结果压缩到[0,1]区间，映射为分类概率，本质是分类模型。
- 公式：线性关系 \(z=w_1x_1+w_2x_2+...+w_nx_n+b\)；Sigmoid函数 \(\sigma(z)=\frac{1}{1+e^{-z}}\)；对数损失函数 \(L(w,b)=-\frac{1}{n}\sum_{i=1}^n[y_i log(\hat{y}_i)+(1-y_i)log(1-\hat{y}_i)]\)。
- 适用场景：垃圾邮件识别、疾病初筛、用户流失预测等二分类场景。

#### 决策树
- 定义：监督学习中兼具分类与回归能力的非线性模型，通过“层层分支”的树形结构实现决策，核心是递归划分特征空间，构建“树状”决策规则。
- 核心节点：根节点（决策起点，对应第一个决策特征）、内部节点（中间决策步骤，对应后续决策特征）、叶节点（决策终点，对应分类结果或回归值）、分支（节点间的连线，对应特征的取值）。
- 适用场景：客户流失预测、疾病诊断、商品分类等，尤其适合需要向业务方解释决策逻辑的场景。

#### K-Means 聚类
- 定义：无监督学习中最经典的聚类算法，核心是将数据集自动划分为K个“簇”（Cluster），使同一簇内样本相似度高，不同簇样本相似度低。
- 公式：欧氏距离 \(d(x_i,x_j)=\sqrt{\sum_{k=1}^n(x_{ik}-x_{jk})^2}\)；SSE（误差平方和）\(SSE=\sum_{k=1}^K\sum_{x_i \in C_k}\|x_i-\mu_k\|^2\)。
- 适用场景：客户分群、文本主题聚类、图像分割等。

#### PCA（主成分分析）
- 定义：无监督学习中最经典的降维算法，核心是通过“线性变换”将高维数据映射到低维空间，在损失少量信息的前提下减少特征数量。
- 核心思路：找到数据中“变异最大的方向”（主成分），将数据投影到这些方向上，用更少的维度概括原始数据的主要信息。
- 适用场景：数据可视化（高维→2D/3D）、模型预处理（减少特征数加速训练）、噪声去除（过滤次要信息）。

#### Apriori 算法
- 定义：无监督学习中发现数据项之间关联关系的技术，核心是从大量交易数据中找出“项集之间的频繁关联”。
- 核心指标：支持度 \(Support(A→B)=\frac{包含A和B的交易数}{总交易数}\)；置信度 \(Confidence(A→B)=\frac{包含A和B的交易数}{包含A的交易数}\)；提升度 \(Lift(A→B)=\frac{Confidence(A→B)}{Support(B)}\)。
- 适用场景：超市商品推荐、电商“买了又买”功能、网页点击路径分析、医疗疾病并发症关联分析等。

#### Q 学习
- 定义：一种基于“动作价值函数（Q函数）”的无模型强化学习算法，通过构建Q表记录每个“状态-动作”对的价值Q(s,a)，利用环境反馈的奖励，迭代更新Q值，最终让智能体学会“在每个状态下选择Q值最大的动作”。
- 公式：贝尔曼更新方程 \(Q(s,a)←Q(s,a)+\alpha[R+\gamma max_{a'}Q(s',a')-Q(s,a)]\)。
- 适用场景：环境转移概率未知的序列决策场景，如迷宫寻宝、简单游戏AI、机器人导航等。

#### DQN（深度Q网络）
- 定义：将“深度神经网络（DNN）”与“Q学习”结合的强化学习算法，用神经网络拟合“状态→动作价值（Q值）”的映射关系，无需存储Q表，可处理高维状态。
- 核心改进：神经网络替代Q表、经验回放（Replay Buffer）、目标网络（Target Network）。
- 适用场景：高维状态空间的序列决策，如Atari游戏AI、机器人控制、自动驾驶等。

### 3. 评估指标概念
#### 分类任务指标
- 准确率（Accuracy）
  - 定义：所有预测正确的样本占总样本的比例。
  - 公式：\(Accuracy=\frac{TP+TN}{TP+TN+FP+FN}\)。
  - 适用场景：样本类别平衡（正负类比例接近1:1）的通用场景，如手写数字识别。
- 精确率（Precision）
  - 定义：预测为正类的样本中，实际为正类的比例。
  - 公式：\(Precision=\frac{TP}{TP+FP}\)。
  - 适用场景：“误判正类成本高”的场景，如垃圾邮件分类、金融反欺诈。
- 召回率（Recall）
  - 定义：实际为正类的样本中，被预测为正类的比例。
  - 公式：\(Recall=\frac{TP}{TP+FN}\)。
  - 适用场景：“漏判正类成本高”的场景，如疾病诊断、火灾预警。
- F1 分数（F1-Score）
  - 定义：精确率与召回率的调和平均，平衡两者矛盾。
  - 公式：\(F1=2×\frac{Precision×Recall}{Precision+Recall}\)。
  - 适用场景：精确率和召回率都需要兼顾的场景，如电商差评识别。
- ROC-AUC
  - 定义：ROC曲线（以“假正例率FPR=FP/(TN+FP)”为横轴，“真正例率TPR=Recall”为纵轴）下的面积，范围[0,1]。
  - 适用场景：需要评估模型“整体区分能力”的场景，如信用评分，对样本不平衡不敏感。

#### 回归任务指标
- MAE（平均绝对误差）
  - 定义：所有样本残差的绝对值的平均值。
  - 公式：\(MAE=\frac{1}{n}\sum_{i=1}^n|y_i-\hat{y}_i|\)。
  - 适用场景：对异常值不敏感的场景，如日常气温预测。
- MSE（均方误差）
  - 定义：所有样本残差的平方的平均值。
  - 公式：\(MSE=\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2\)。
  - 适用场景：对大误差敏感的场景，如股票价格预测。
- RMSE（均方根误差）
  - 定义：MSE的平方根。
  - 公式：\(RMSE=\sqrt{MSE}=\sqrt{\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2}\)。
  - 适用场景：需要兼顾“惩罚大误差”和“单位直观”的场景，如电商销量预测。
- \(R^2\)（决定系数）
  - 定义：衡量模型解释数据变异的能力。
  - 公式：\(R^2=1-\frac{\sum(y_i-\hat{y}_i)^2}{\sum(y_i-\bar{y})^2}\)。
  - 适用场景：需要评估模型“解释力”的场景，如经济学中GDP影响因素分析。

#### 聚类任务指标
- 轮廓系数（Silhouette Coefficient）
  - 定义：对每个样本计算簇内平均距离a和簇间最小平均距离b，单个样本轮廓系数为(b-a)/max(a,b)，整体为所有样本的平均值，范围[-1,1]。
  - 适用场景：通用聚类评估，如用户分群，对球形簇效果好。
- DB指数（Davies-Bouldin Index, DBI）
  - 定义：衡量簇内紧密度和簇间距离的综合指标，DBI越小越好。
  - 公式：\(DBI=\frac{1}{k}\sum_{i=1}^k max_{j≠i}\frac{S_i+S_j}{M_{ij}}\)。
  - 适用场景：需要比较不同簇数效果的场景，如商品分类。

#### 生成任务指标
- 困惑度（Perplexity, PPL）
  - 定义：衡量语言模型预测文本的不确定性，PPL越低，模型预测文本越准确。
  - 公式：基础定义式 \(PPL(W)=(\frac{1}{P(w_1,w_2,...,w_N)})^{\frac{1}{N}}\)；对数简化式 \(log(PPL(W))=-\frac{1}{N}\sum_{i=1}^N log(P(w_i|w_1,...,w_{i-1}))\)。
  - 适用场景：语言生成模型，如文本续写、对话系统。
- FID（弗雷歇Inception距离）
  - 定义：通过Inception神经网络提取特征后，计算“真实图像特征分布”与“生成图像特征分布”的弗雷歇距离。
  - 适用场景：图像生成模型，如Stable Diffusion、GAN，FID值越小生成质量越高。
- IS（Inception分数）
  - 定义：基于Inception神经网络类别预测结果，衡量“生成图像的多样性”和“单张生成图像的真实性”。
  - 适用场景：图像生成模型，如文生图、图像修复，IS数值越高生成质量越好。

## 三、简答题
1. 机器学习与传统编程的核心区别是什么？
   - 传统编程：输入“规则+数据”，输出“结果”，依赖人工设计规则，复杂场景难以覆盖。
   - 机器学习：输入“数据+结果”，输出“规则（模型）”，自动从数据中学习规则，适配复杂场景。

2. 监督学习的优势和局限分别是什么？
   - 优势：预测精度高、结果可解释性强（如决策树）。
   - 局限：依赖大量人工标注数据（成本高）、对标签噪声敏感（标注错误会影响模型）。

3. 无监督学习的典型算法和应用场景有哪些？
   - 典型算法：聚类任务（K-Means）、降维任务（PCA）、关联规则（Apriori）。
   - 应用场景：聚类（客户分群、异常检测）、降维（高维数据可视化、特征压缩）、关联规则（超市购物篮分析）。

4. 机器学习的一般流程包含哪些关键步骤？
   - 数据收集与预处理（数据清洗、数据转换、数据划分）。
   - 模型选择与训练（模型选型、喂入训练集、损失函数计算误差、优化算法更新参数）。
   - 模型评估与调优（选择评估指标、超参数调优、模型改进）。
   - 模型部署/迭代优化。

5. 数据预处理中缺失值和异常值的常见处理方法是什么？
   - 缺失值处理：数值型（均值/中位数填充）；类别型（众数填充、新增“未知”类别）。
   - 异常值处理：通过箱线图（IQR法则）或Z-score检测（|Z|>3为异常），选择删除、修正或保留。

6. 线性回归的常见问题及解决办法有哪些？
   - 欠拟合（R²低，预测误差大）：增加相关特征；考虑非线性关系（如多项式回归）。
   - 异常值干扰（模型拟合线偏离大部分数据）：用箱线图识别异常值，删除或修正。
   - 特征量纲不一致（权重受量纲影响）：对特征标准化（StandardScaler）。

7. 决策树防止过拟合的剪枝策略有哪些？
   - 预剪枝：建树时限制条件，如设置最大深度max_depth、最小样本数min_samples_leaf，简单高效但可能欠拟合。
   - 后剪枝：先建完整树，再从叶子节点向上剪枝，对分支后性能无提升的子树，剪枝为叶节点，效果更好但计算量大。

8. K-Means聚类的常见问题及解决办法是什么？
   - K值难确定：用肘部法（看SSE突变点）或轮廓系数；结合业务场景（如客户分群通常3-5类）。
   - 初始质心敏感（多次运行结果不同）：增加n_init参数，用K-Means++算法优化初始质心选择。
   - 对异常值敏感（异常值拉偏质心）：聚类前用箱线图或Z-score去除异常值。
   - 不适用于非凸簇：用DBSCAN等密度聚类算法替代。

9. 强化学习的六大核心要素是什么？
   - 智能体（Agent）：做出决策、执行动作、学习策略的主体。
   - 环境（Environment）：智能体所处的外部场景，会对智能体的动作做出反馈。
   - 状态（State, S）：智能体在环境中的当前处境。
   - 动作（Action, A）：智能体在当前状态下可执行的操作。
   - 奖励（Reward, R）：环境对智能体动作的即时反馈（正奖励鼓励，负奖励惩罚）。
   - 策略（Policy, π）：智能体从“状态→动作”的映射规则。

10. Q学习的五大步骤是什么？
    - 步骤1：初始化Q表与参数（学习率α、探索率ε、折扣因子γ等）。
    - 步骤2：智能体选择动作（ε-贪心策略，r<ε随机选动作，r≥ε选当前状态Q值最大的动作）。
    - 步骤3：执行动作，获取环境反馈（新状态s'、即时奖励R，若为终止状态则该轮结束）。
    - 步骤4：用贝尔曼方程更新Q表（计算目标Q值，更新当前Q(s,a)）。
    - 步骤5：更新状态，重复迭代（直到完成设定迭代次数或Q表收敛）。

## 四、计算题
### 1. 垃圾邮件识别分类指标计算
#### 题目：已知总邮件1000封，真实垃圾邮件（正类）100封，真实正常邮件（负类）900封。模型预测结果：TP=75（正确识别的垃圾邮件），FP=30（误判为垃圾邮件的正常邮件），FN=25（漏判的垃圾邮件），TN=875（正确识别的正常邮件）。计算准确率、精确率、召回率和F1值。
#### 计算步骤：
- 准确率：\(Accuracy=\frac{TP+TN}{TP+TN+FP+FN}=\frac{75+875}{1000}=0.95=95\%\)。
- 精确率：\(Precision=\frac{TP}{TP+FP}=\frac{75}{75+30}≈0.714=71.4\%\)。
- 召回率：\(Recall=\frac{TP}{TP+FN}=\frac{75}{75+25}=0.75=75.0\%\)。
- F1值：\(F1=2×\frac{Precision×Recall}{Precision+Recall}=2×\frac{0.714×0.75}{0.714+0.75}≈0.732=73.2\%\)。
#### 结果：准确率95%，精确率71.4%，召回率75.0%，F1值73.2%。

### 2. 疾病诊断分类指标计算（场景A）
#### 题目：总人数1000人，真实患病（正类）1人，真实健康（负类）999人。模型“偷懒”全预测为健康，预测结果：TP=0，FP=0，FN=1，TN=999。计算准确率、精确率、召回率和F1值。
#### 计算步骤：
- 准确率：\(Accuracy=\frac{TP+TN}{总样本}=\frac{0+999}{1000}=0.999=99.9\%\)。
- 精确率：\(Precision=\frac{TP}{TP+FP}=\frac{0}{0+0}\)（无意义）。
- 召回率：\(Recall=\frac{TP}{TP+FN}=\frac{0}{0+1}=0\% \)。
- F1值：因无正类预测，无意义。
#### 结果：准确率99.9%，精确率无意义，召回率0%，F1值无意义。

### 3. 疾病诊断分类指标计算（场景B）
#### 题目：总人数1000人，真实患病（正类）1人，真实健康（负类）999人。模型“谨慎预测”10人患病（含1个真患者），预测结果：TP=1，FP=9，FN=0，TN=990。计算准确率、精确率、召回率和F1值。
#### 计算步骤：
- 准确率：\(Accuracy=\frac{TP+TN}{总样本}=\frac{1+990}{1000}=0.991=99.1\%\)。
- 精确率：\(Precision=\frac{TP}{TP+FP}=\frac{1}{1+9}=0.1=10\%\)。
- 召回率：\(Recall=\frac{TP}{TP+FN}=\frac{1}{1+0}=1=100\%\)。
- F1值：\(F1=2×\frac{Precision×Recall}{Precision+Recall}=2×\frac{0.1×1}{0.1+1}≈0.182=18.2\%\)。
#### 结果：准确率99.1%，精确率10%，召回率100%，F1值18.2%。

### 4. 线性回归损失函数计算
#### 题目：现有3个房价预测样本，真实值y分别为100万元、150万元、200万元，模型预测值\(\hat{y}\)分别为90万元、160万元、190万元。计算该模型的MSE（均方误差）。
#### 计算步骤：
- 第一步：计算每个样本的残差（真实值-预测值）：100-90=10，150-160=-10，200-190=10。
- 第二步：计算残差的平方：\(10^2=100\)，\((-10)^2=100\)，\(10^2=100\)。
- 第三步：计算平方和的平均值：\(MSE=\frac{100+100+100}{3}=\frac{300}{3}=100\)（万元²）。
#### 结果：MSE=100万元²。

### 5. K-Means聚类SSE计算（简化案例）
#### 题目：假设K=2，簇1包含样本[1,3,5]，质心μ₁=3；簇2包含样本[2,4,6]，质心μ₂=4。计算该聚类结果的SSE（误差平方和）。
#### 计算步骤：
- 第一步：计算簇1内每个样本到质心的平方距离：\((1-3)^2=4\)，\((3-3)^2=0\)，\((5-3)^2=4\)，簇1平方和=4+0+4=8。
- 第二步：计算簇2内每个样本到质心的平方距离：\((2-4)^2=4\)，\((4-4)^2=0\)，\((6-4)^2=4\)，簇2平方和=4+0+4=8。
- 第三步：计算SSE：SSE=簇1平方和+簇2平方和=8+8=16。
#### 结果：SSE=16。

## 结尾交付物提议
要不要我帮你整理一份**机器学习核心公式汇总表**，包含所有关键算法和评估指标的公式、符号说明及适用场景，方便你快速查阅和记忆？
