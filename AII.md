# 机器学习框架结构（基于PPT原文）
## 1 机器学习概述
### 1.1 机器学习的定义与任务
#### 1.1.1 定义
- 让计算机从数据中自动学习规律，无需显式编程即可完成任务的技术
- 本质：通过数据驱动优化模型参数，实现对未知数据的预测或决策，即“泛化能力”
- 与传统编程的区别：
  | 传统编程 | 机器学习 |
  | --- | --- |
  | 输入：规则+数据→输出：结果 | 输入：数据+结果→输出：规则（模型） |
  | 依赖人工设计规则，复杂场景难以覆盖 | 自动从数据中学习规则，适配复杂场景 |

#### 1.1.2 基本任务分类
| 任务类型 | 目标 | 典型场景 |
| --- | --- | --- |
| 预测任务 | 基于历史数据预测未来结果 | 房价预测（回归）、垃圾邮件识别（分类） |
| 描述任务 | 挖掘数据内在结构或关联 | 用户分群（聚类）、商品关联推荐（关联规则） |

#### 1.1.3 四大学习策略
##### （1）监督学习（Supervised Learning）--“有老师教的学习”
- 定义：利用带标签的训练数据（输入→输出的明确映射），让模型学习“输入到标签的映射规则”，最终实现对新数据的标签预测
- 数据特点：有明确“输入-输出标签对”，如“图像→猫/狗”，“特征→房价”，标签需人工标注，数据质量要求高
- 学习逻辑：通过最小化“预测值与真实标签的误差”，反向调整模型参数，拟合映射规则。类比“学生做带答案的习题，修正错误”
- 典型算法：线性回归、逻辑回归、决策树
- 应用场景：有明确预测目标的场景
  - 分类：垃圾邮件识别、疾病诊断（良性/恶性）、图像识别（猫/狗）
  - 回归：房价预测、股票涨幅预测、销量预估
- 优势：预测精度高、结果可解释性强（如决策树）
- 局限：依赖大量人工标注数据（成本高）、对标签噪声敏感（标注错误会影响模型）

##### （2）无监督学习（Unsupervised Learning）--“自学成才的学习”
- 定义：仅利用无标签的训练数据，让模型自主挖掘数据的“内在结构或规律”，如聚类、降维，无需人工指定输出目标
- 数据特点：只有输入数据，无任何标签，如“一堆用户行为数据”，“一批未分类的图像”，无需人工标注
- 学习逻辑：模型通过分析数据的统计特征，如距离、密度、方差，发现数据的自然分组或冗余信息，类比“学生观察一堆无序数据，自己找规律”
- 典型算法：
  - 聚类任务：K-Means
  - 降维任务：PCA
  - 关联规则：Apriori
- 应用场景：探索数据规律的场景
  - 聚类：客户分群（高价值/普通用户）、异常检测（信用卡欺诈）
  - 降维：高维数据可视化（如基因数据降维）、特征压缩
  - 关联规则：超市购物篮分析（“买面包的人常买牛奶”）
- 优势：无需标注数据（成本低）、可发现未知规律（如隐藏客户群体）
- 局限：结果解释性弱（如聚类结果需人工解读）、对数据分布敏感

##### （3）半监督学习（Semi-Supervised Learning）--“少量老师+大量自学”
- 定义：结合少量带标签数据和大量无标签数据训练模型，利用无标签数据的结构信息辅助标签学习，解决“标签稀缺”问题
- 数据特点：少量标签数据（10%~30%）+大量无标签数据（70%~90%），标签标注成本低
- 学习逻辑：先通过无标签数据挖掘数据结构（如聚类），再用少量标签“指导”模型优化映射规则，类比“学生先自学教材，再找老师答疑重点”
- 典型算法：标签传播（Label Propagation）、标签扩散（Label Spreading）、半监督SVM
- 应用场景：标签标注成本高的场景
  - 文本分类（如行业文档分类，仅标注少量样本）
  - 图像识别（如医疗影像分类，标注需专业医生）
  - 语音识别（方言语音标注难度大）
- 优势：平衡标注成本与预测精度，适配低资源场景
- 局限：对数据分布一致性要求高（无标签数据与标签数据需同分布）

##### （4）强化学习（Reinforcement Learning）--“从试错中学习”
- 定义：智能体（Agent）通过与环境持续交互，接收环境反馈的“奖励/惩罚信号”，逐步学习“最大化累计奖励”的最优策略（动作选择规则）
- 数据特点：无标签数据，有“环境-动作-奖励”的交互序列（如“游戏状态→操作→得分”），数据通过交互生成
- 学习逻辑：智能体执行动作→环境返回新状态+奖励→模型更新策略（奖励为正强化、惩罚为负强化），迭代优化，类比“机器人尝试走路，摔倒=惩罚，走稳=奖励，逐步学会平衡”
- 典型算法：Q-Learning、DQN（深度Q网络）
- 应用场景：序列决策与交互场景
  - 游戏AI
  - 机器人控制（仓储机器人导航、机械臂抓取）
  - 自动驾驶（车道保持、避障决策）
  - 资源调度（电网负荷优化）
- 优势：适配动态环境、无需先验数据（从交互中学习）
- 局限：训练周期长（需大量交互）、奖励函数设计难度高（奖励不合理会导致策略失效）

### 1.2 机器学习的一般流程
#### 1.2.1 数据收集与预处理
- 数据收集
  - 来源：公开数据集（Kaggle、UCI）、业务系统日志、爬虫采集（需合规）
  - 案例：房价预测数据（特征：面积、卧室数、地段；标签：房价）
- 数据清洗
  - 缺失值处理：数值型（均值/中位数填充，如“面积缺失用均值填充”）；类别型（众数填充、新增“未知”类别）
  - 异常值处理：通过箱线图（IQR法则）或Z-score检测（|Z|>3为异常），选择删除、修正或保留（如“房价1亿”明显异常，需核实后删除）
  - Z-score检测公式：\(Z=(X-\mu) / \sigma\)（X：数据点；μ：数据集均值；σ：数据集标准差；判定标准：通常以|Z|>3为异常阈值）
- 数据转换
  - 类别变量编码：One-Hot编码（如“地段：城东→[1,0,0]，城西→[0,1,0]”）、标签编码（有序类别，如“楼层：低→1，中→2，高→3”）
  - 数值标准化/归一化：
    - 标准化（Z-Score）：\(x'=\frac{x-\mu}{\sigma}\)（适用于正态分布数据，如“面积”）
    - 归一化（Min-Max）：\(x'=\frac{x-x_{min }}{x_{max }-x_{min }}\)（适用于非正态分布，输出范围[0,1]）
- 数据划分
  - 训练集（70%~80%）：用于模型参数学习
  - 验证集（10%~15%）：用于超参数调优（如“学习率选择”）
  - 测试集（10%~15%）：模拟真实场景，评估最终模型泛化能力（不可用于模型调优）

#### 1.2.2 模型选择与训练
- 模型选型原则
  - 简单任务优先用简单模型，如“线性关系房价预测”用线性回归，而非复杂的神经网络
  - 复杂任务结合数据规模，如“百万级图像分类”用CNN，而非决策树
- 训练过程
  - 喂入训练集数据，通过“损失函数”计算预测值与真实值的误差
  - 用“优化算法”更新模型参数，最小化损失函数，如线性回归的梯度下降

#### 1.2.3 模型评估与调优
- 评估指标（按任务选）
  - 回归任务（如房价预测）：MSE（均方误差）、MAE（平均绝对误差）、\(R^{2}\)（决定系数，越接近1越好）
  - 分类任务（如垃圾邮件识别）：准确率、精确率、召回率、F1值、ROC-AUC（不平衡数据优先看F1或AUC）
- 调优方法
  - 超参数调优：网格搜索（遍历参数组合，如“学习率0.001/0.01/0.1 + 树深度3/5/7”）、随机搜索（随机采样参数，效率更高）
  - 模型改进：增加特征、处理过拟合（正则化、剪枝）、集成学习（如随机森林）

### 1.3 机器学习模型的评估指标
#### 1.3.1 分类任务指标（适用于“二分类/多分类”，如垃圾邮件识别、疾病诊断）
- 混淆矩阵基础
  - TP（True Positive）：实际正类，预测正类
  - TN（True Negative）：实际负类，预测负类
  - FP（False Positive）：实际负类，预测正类（误判为正）
  - FN（False Negative）：实际正类，预测负类（漏判为负）
- 准确率（Accuracy）
  - 定义：所有预测正确的样本占总样本的比例，公式：\(Accuracy =\frac{TP+TN}{TP+TN+FP+FN}\)
  - 适用场景：样本类别平衡（如正负类比例接近1:1）的通用场景，如手写数字识别（10类样本分布均匀）
  - 局限：样本不平衡时失效（如癌症诊断中，99%为健康人，模型全预测为“健康”也能得99%准确率，但无实际意义）
- 精确率（Precision，也称查准率）
  - 定义：预测为正类的样本中，实际为正类的比例（关注“预测正类的准确性”），公式：\(Precision =\frac{TP}{TP+FP}\)
  - 适用场景：“误判正类成本高”的场景，如垃圾邮件分类（避免将正常邮件判为垃圾邮件，FP代价高）、金融反欺诈（避免将正常交易判为欺诈，影响用户体验）
  - 特点：优先控制“误判为正”的错误，代价是可能漏判正类（FN升高）
- 召回率（Recall，也称查全率）
  - 定义：实际为正类的样本中，被预测为正类的比例（关注“正类是否被全找到”），公式：\(Recall =\frac{TP}{TP+FN}\)
  - 适用场景：“漏判正类成本高”的场景，如疾病诊断（避免漏诊癌症患者，FN代价高）、火灾预警（避免漏报火灾，造成严重损失）
  - 特点：优先控制“漏判为负”的错误，代价是可能误判更多负类（FP升高）
- F1分数（F1-Score）
  - 定义：精确率与召回率的调和平均，平衡两者矛盾，公式：\(F1=2 × \frac{Precision × Recall}{Precision + Recall}\)
  - 适用场景：精确率和召回率都需要兼顾的场景，如电商差评识别：既不想把好评判为差评--控制FP，也不想漏掉差评--控制FN
  - 特点：F1越接近1，模型综合性能越好；若精确率或召回率极低，F1会显著下降（调和平均对极端值敏感）
- ROC曲线与AUC（ROC-AUC）
  - 定义：
    - ROC（Receiver Operating Characteristic）曲线：以“假正例率（FPR=FP/(TN+FP)）”为横轴，“真正例率（TPR=Recall）”为纵轴的曲线，反映模型在不同阈值下的区分能力
    - AUC（Area Under ROC Curve）：ROC曲线下的面积，范围[0,1]，AUC=1表示完美区分正负类，AUC=0.5表示模型无区分能力（等同于随机猜测）
  - 适用场景：需要评估模型“整体区分能力”的场景，如信用评分（判断用户是否违约，需在不同风险阈值下稳定区分）
  - 特点：对样本不平衡不敏感，适合正负类比例差异大的场景（如罕见疾病诊断）
- PR曲线与AUC（PR-AUC）
  - 定义：
    - PR曲线：以“召回率”为横轴，“精确率”为纵轴的曲线，反映两者在不同阈值下的权衡
    - PR-AUC：PR曲线下的面积，范围[0,1]，越接近1表示模型在精确率和召回率上的平衡越好
  - 适用场景：正样本极罕见的场景（如欺诈交易识别，正样本占比<1%），此时ROC-AUC可能偏高（因TN多，FPR低），但PR-AUC更能反映模型对正样本的捕捉能力
  - 局限：负样本极少时PR曲线不稳定（如正样本占比90%，PR-AUC参考价值低）

#### 1.3.2 回归任务指标（适用于“预测连续值”，如房价预测、销量预测）
- 平均绝对误差（MAE，Mean Absolute Error）
  - 定义：所有样本残差的绝对值的平均值，公式：\(MAE=\frac{1}{n} \sum_{i=1}^{n}\left|y_{i}-\hat{y}_{i}\right|\)（\(y_{i}\)为真实值，\(\hat{y}_{i}\)为预测值，n为样本数）
  - 适用场景：对异常值不敏感的场景，如日常气温预测（偶尔极端高温/低温不会显著影响MAE）
  - 特点：单位与原数据一致（如房价预测中MAE=5万元，直观表示平均误差5万），但未惩罚大误差
- 均方误差（MSE，Mean Squared Error）
  - 定义：所有样本残差的平方的平均值，公式：\(MSE=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}\)
  - 适用场景：对大误差敏感的场景，如股票价格预测（大误差会导致严重亏损，需重点惩罚）
  - 特点：平方项会放大异常值的误差（如残差=10时，平方后=100，对MSE影响远大于残差=1的样本）
  - 局限：单位是原数据的平方（如房价预测中MSE=25万元²，不直观）
- 均方根误差（RMSE，Root Mean Squared Error）
  - 定义：MSE的平方根，公式：\(RMSE =\sqrt{MSE}=\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}}\)
  - 适用场景：需要兼顾“惩罚大误差”和“单位直观”的场景，如电商销量预测（既关注大误差，又需直观理解误差大小）
  - 特点：继承MSE对大误差的敏感性，同时单位与原数据一致（如销量预测中RMSE=100件，直观表示平均误差100件）
- 决定系数（\(R^{2}\)，R-Squared）
  - 定义：衡量模型解释数据变异的能力，公式：\(R^{2}=1-\frac{\sum(y_{i}-\hat{y}_{i})^{2}}{\sum(y_{i}-\bar{y})^{2}}\)（\(\bar{y}\)为真实值的平均值）
  - 含义：\(R^{2}=1\)表示模型完美拟合所有数据；\(R^{2}=0\)表示模型预测等同于“用平均值预测”；\(R^{2}<0\)表示模型预测比“用平均值”更差
  - 适用场景：需要评估模型“解释力”的场景，如经济学中GDP影响因素分析（\(R^{2}=0.8\)表示模型能解释80%的GDP变异）
  - 局限：添加无关特征时\(R^{2}\)可能虚高，需结合调整后\(R^{2}\)（Adjusted \(R^{2}\)）使用

#### 1.3.3 聚类任务指标（适用于“无监督聚类”，如用户分群、商品分类）
- 轮廓系数（Silhouette Coefficient）
  - 定义：对每个样本计算
    - 簇内平均距离a（样本与同簇其他样本的平均距离）
    - 簇间最小平均距离b（样本与最近异簇所有样本的平均距离）
    - 单个样本轮廓系数为(b-a)/max(a,b)
    - 整体轮廓系数为所有样本的平均值，范围[-1,1]
  - 含义：越接近1，说明样本在簇内越紧凑、与其他簇越分离，聚类效果越好；接近-1表示样本分错簇；接近0表示簇边界模糊
  - 适用场景：通用聚类评估，如用户分群（判断分群是否清晰）
  - 局限：对球形簇效果好，对非球形簇（如长条簇）评估不准
- DB指数（Davies-Bouldin Index，DBI）
  - 簇内平均距离（\(S_{i}\)）：衡量簇i中所有数据点到簇中心的平均距离，反映簇内数据的分散程度，公式：\(S_{i}=\frac{1}{T_{i}} \sum_{j=1}^{T_{i}}\left\|X_{j}-A_{i}\right\|\)（\(X_{j}\)为簇i的第j个数据点，\(A_{i}\)为簇i的质心，\(T_{i}\)为簇i的数据点数量，||·||为欧氏距离）
  - 簇间距离（\(M_{ij}\)）：簇i与簇j的质心之间的欧氏距离，公式：\(M_{ij}=\left\|A_{i}-A_{j}\right\|\)
  - 相似度（\(R_{ij}\)）：结合簇内紧密度和簇间距离，衡量簇i与簇j的相似度，公式：\(R_{ij}=\frac{S_{i}+S_{j}}{M_{ij}}\)
  - DBI计算公式：\(DBI=\frac{1}{k} \sum_{i=1}^{k} max _{j \neq i} R_{ij}\)（k为簇数）
  - 含义：DBI越小越好，越小表示簇内越紧凑、簇间越远
  - 适用场景：需要比较不同簇数效果的场景，如商品分类（通过DBI选择最优簇数k）
  - 特点：无需知道真实簇标签，纯无监督评估

#### 1.3.4 生成任务指标（适用于“生成式模型”，如文生图、文本生成）
- 困惑度（Perplexity，PPL）
  - 定义：衡量语言模型预测文本的不确定性，公式为“模型对文本序列概率的倒数的几何平均”，简化理解为：PPL越低，模型预测文本越准确（越不“困惑”）
  - 基础定义式（几何平均形式）：\(PPL(W)=\left(\frac{1}{P\left(w_{1}, w_{2}, ..., w_{N}\right)}\right)^{\frac{1}{N}}\)（W为长度为N的离散文本序列）
  - 基于链式法则的展开式（实际计算常用）：\(PPL(W)=\left(\frac{1}{\prod_{i=1}^{N} P\left(w_{i} | w_{1}, w_{2}, ..., w_{i-1}\right)}\right)^{\frac{1}{N}}\)
  - 对数简化式（工程计算防下溢）：\(log (PPL(W))=-\frac{1}{N} \sum_{i=1}^{N} log \left(P\left(w_{i} | w_{1}, ..., w_{i-1}\right)\right)\)，最终PPL为对数结果的指数
  - 适用场景：语言生成模型（如GPT、LLaMA），如文本续写、对话系统
  - 特点：PPL=10表示模型对每个token的预测有10种可能选择，PPL=2表示预测几乎确定
  - 局限：仅关注文本概率，不直接衡量语义连贯性
- 弗雷歇inception距离（FID，Fréchet Inception Distance）
  - 定义：通过Inception神经网络提取特征后，计算“真实图像特征分布”与“生成图像特征分布”的弗雷歇（Fréchet）距离，反映两类图像在高层语义特征上的相似性。是衡量生成式图像模型（如Stable Diffusion、GAN）生成质量的主要指标
  - 弗雷歇距离的本质：衡量“两个概率分布”相似性的指标，尤其适用于高维高斯分布（图像特征经Inception提取后，近似服从高斯分布）。其思想是“找到两个分布间的最优路径，计算路径上的最大距离”，数学上简化为“均值距离+协方差距离”
  - 含义：FID值越小，说明生成图像与真实图像的特征分布越接近，生成质量越高；典型参考值：FID=5（生成质量极高，接近真实）、FID=20（优秀）、FID=50（一般）、FID>100（生成质量差，存在模糊/失真）
  - 适用场景：图像生成模型（如Stable Diffusion、Midjourney）
  - 局限：对图像风格差异敏感，不适合评估抽象艺术生成
- inception分数（IS，Inception Score）
  - 定义：基于Inception神经网络类别预测结果，从“生成图像的类别预测概率”出发，衡量“生成图像集的多样性（不同图像预测类别差异大）”和“单张生成图像的真实性（单张图像类别预测概率集中）”，IS数值越高表示生成质量越好
  - 数值意义：理论上1≤IS≤C（C为类别数，如ImageNet的C=1000）；实际参考值：优秀的生成模型（如StyleGAN2）在人脸数据集上IS≈10~20；文生图模型（如Stable Diffusion）在通用图像上IS≈20~50；IS<5表示生成图像模糊/类别混淆，质量差
  - 适用场景：图像生成模型，如文生图、图像修复
  - 特点：IS高表示生成图像既真实（类别明确）又多样（类别不重复）
  - 局限：对小样本生成评估不稳定，易受类别不平衡影响

#### 1.3.5 指标选择原则
- 匹配任务类型：
  - 分类用Accuracy/F1/ROC-AUC
  - 回归用MAE/RMSE/R²
  - 聚类用轮廓系数/DBI
  - 生成用FID/PPL
- 关注业务成本：
  - 误判成本高选Precision/Recall
  - 漏判成本高选Recall
  - 平衡选F1
- 避免单一指标：
  - 分类任务需结合“Accuracy+F1+ROC-AUC”
  - 回归任务需结合“RMSE+R²”

## 2 统计机器学习
### 2.1 监督学习
#### 2.1.1 线性回归（回归任务基准模型）
- 概念
  - 定义：线性回归假设自变量（特征）与因变量（目标值）之间存在线性相关关系，通过学习数据中的规律，构建线性函数来拟合数据，进而实现预测
  - 两种形式：
    - 单变量线性回归：\(y=wx+b\)，适用于单个特征预测目标值，例如用房屋面积(x)预测房价(y)
    - 多变量线性回归：\(y=w_{1}x_{1}+w_{2}x_{2}+...+w_{n}x_{n}+b\)，适用于多个特征共同预测，例如用面积x₁、楼层x₂、房龄x₃预测房价y
  - 符号说明：
    - y：因变量（待预测的连续值）
    - \(x_{1},x_{2},...,x_{n}\)：自变量（输入特征）
    - \(w_{1},w_{2},...,w_{n}\)：特征权重（表示对应特征对y的影响程度）
    - b：偏置项（修正特征线性组合与实际y的偏差）
  - 线性回归需满足3个基础前提：
    - 自变量与因变量线性相关
    - 观测数据之间相互独立，残差（预测值与实际值的差异）无相关性
    - 残差呈正态分布
- 原理
  - 学习目标：最小化预测误差，找到一组最优的w和b，让模型的预测值\(\hat{y}\)（模型输出）尽可能接近真实值y
    - 预测值：\(\hat{y}=wx+b\)（单变量）
    - 误差（残差）：\(e=y-\hat{y}\)（真实值与预测值的差值）
  - 损失函数：量化误差（平方损失函数），公式：\(L(w, b)=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\left(w x_{i}+b\right)\right)^{2}\)（n为样本数量，平方操作可放大较大误差，同时避免正负误差抵消）
  - 参数求解：最小二乘法（解析解），通过求导找极值确定最优w和b（无需迭代，直接通过公式计算）
- 评估指标
  - MSE（均方误差）：\(MSE=\frac{1}{m} \sum_{i=1}^{m}(y_{i}-\hat{y}_{i})^{2}\)（单位：平方）
  - MAE（平均绝对误差）：\(MAE=\frac{1}{m} \sum_{i=1}^{m} |y_{i}-\hat{y}_{i}|\)（单位：原单位，如万，更直观）
  - \(R^{2}\)（决定系数）：\(R^{2}=1-\frac{\sum(y_{i}-\hat{y}_{i})^{2}}{\sum(y_{i}-\bar{y})^{2}}\)（越接近1越好）
- 常见问题与解决办法
  | 常见问题 | 表现 | 解决办法 |
  | --- | --- | --- |
  | 欠拟合（拟合效果差） | 决定系数\(R^{2}\)很低，预测误差大 | 增加相关特征；考虑非线性关系（如多项式回归） |
  | 异常值干扰 | 模型拟合线偏离大部分数据 | 用箱线图识别异常值，删除或修正 |
  | 特征量纲不一致 | 权重受量纲影响（如面积用㎡和ft²） | 对特征标准化（StandardScaler） |

#### 2.1.2 逻辑回归（分类任务常用模型）
- 概念
  - 定义：逻辑回归是监督学习二分类任务的基准模型，虽名字带“回归”，但本质是分类模型；与线性回归的差异：线性回归预测连续值（如房价），逻辑回归输出分类概率（如“是垃圾邮件的概率0.9”）
  - 适用场景：垃圾邮件识别、疾病初筛、用户流失预测等二分类场景
  - 思路：从线性结果到分类概率
    - 先构建线性关系：\(z=w_{1}x_{1}+w_{2}x_{2}+...+w_{n}x_{n}+b\)（与多变量线性回归形式一致）
    - 代入Sigmoid函数：\(\sigma(z)=\frac{1}{1+e^{-z}}\)，输出值\(\hat{y}\)即为“样本属于正类的概率”
  - Sigmoid函数的关键特性：图像呈“S”形，当z=0时，\(\sigma(z)=0.5\)；阈值规则：通常设0.5为分界点，\(\hat{y}≥0.5\)预测为正类（如“垃圾邮件”），\(\hat{y}<0.5\)预测为负类（如“正常邮件”）
  - 模型参数含义：
    - \(w_{i}\)（特征权重）：正数表示该特征提升正类概率，负数则降低，如垃圾邮件识别中，“含‘优惠’关键词”的w为正，且数值越大，越易判定为垃圾邮件
    - b（偏置项）：调整整体概率基准，例如b为正，即使无特征输入，样本也有基础正类概率
- 原理：模型如何“学习”最优参数？
  - 损失函数：
    - 对数损失函数（交叉熵损失）--二分类场景：\(L(w, b)=-\frac{1}{n} \sum_{i=1}^{n}\left[y_{i} log \left(\hat{y}_{i}\right)+\left(1-y_{i}\right) log \left(1-\hat{y}_{i}\right)\right]\)（\(y_{i}\)为样本真实标签（1=正类，0=负类），\(\hat{y}_{i}\)为模型预测的正类概率；核心逻辑：若样本是正类（\(y_{i}=1\)），\(\hat{y}_{i}\)越接近1，损失越小；若为负类（\(y_{i}=0\)），\(\hat{y}_{i}\)越接近0，损失越小）
    - Softmax回归--多分类场景：原理是将二分类推广到多分类，输出每个类别的概率（和为1）；Softmax函数：\(\sigma(z)_{k}=\frac{e^{z_{k}}}{\sum_{j=1}^{K} e^{z_{j}}}\)（K为类别数，\(z_{k}\)为第k类的线性输出）；损失函数：\(L(w)=-\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} y_{ik} log(\sigma(z_{ik}))\)（\(y_{ik}=1\)表示样本i属于第k类）
  - 参数求解：梯度下降法，通过迭代调整w和b，逐步减小对数损失函数值；通俗理解：像下山一样，每次朝着损失减小最快的方向走一小步（步长即学习率），直到走到山底（损失最小）；无需手动计算，sklearn库会自动完成迭代求解
- 分类任务评估指标
  | 指标 | 定义 | 适用场景 |
  | --- | --- | --- |
  | 准确率(Accuracy) | 预测正确的样本占总样本比例 | 类别平衡场景（如正负类比例1:1） |
  | 精确率(Precision) | 预测为正类的样本中真实正类的比例 | 误判正类代价高（如疾病诊断避免误诊健康人） |
  | 召回率(Recall) | 真实正类中被正确预测的比例 | 漏判正类代价高（如癌症诊断避免漏诊患者） |
  | F1分数 | 精确率与召回率的调和平均 | 需平衡精确率和召回率的场景 |
  | 混淆矩阵 | 直观展示TP、TN、FP、FN的数量 | 快速定位模型误判类型 |

#### 2.1.3 决策树（可解释性强的非线性模型）
- 概念：决策树的“树形结构”
  - 定义：决策树是监督学习中兼具分类与回归能力的非线性模型，通过“层层分支”的树形结构实现决策
  - 核心节点分类（以“是否放贷”为例）：
    | 节点类型 | 定义 | 示例 |
    | --- | --- | --- |
    | 根节点 | 决策树的起点，对应第一个决策特征 | “月收入是否≥8000元” |
    | 内部节点 | 中间决策步骤，对应后续决策特征 | “征信是否无逾期” |
    | 叶节点 | 决策终点，对应分类结果或回归值 | “同意放贷”“拒绝放贷” |
    | 分支 | 节点间的连线，对应特征的取值 | “月收入≥8000元”→“是/否”两个分支 |
  - 决策树的两类任务：
    - 分类决策树（如CART、ID3、C4.5）：叶节点为类别标签（如“放贷/不放贷”）
    - 回归决策树（如CART）：叶节点为连续值（如预测房价具体金额）
  - 优势：可解释性极强（能清晰看到每一步决策依据），无需特征标准化，对异常值不敏感
  - 适用场景：客户流失预测、疾病诊断、商品分类等，尤其适合需要向业务方解释决策逻辑的场景
- 原理：递归划分特征空间，构建“树状”决策规则
  - 特征选择：选“区分度最高”的特征先分支，目标是每次分支后，数据的“纯度”更高（同类样本尽量聚集），常用3类指标：
    | 指标 | 特征类型 | 适用模型 | 核心逻辑 | 特点 |
    | --- | --- | --- | --- | --- |
    | 信息增益（ID3算法） | 离散特征 | 分类 | 最大化信息增益，降低不确定性（信息熵：\(H(D)=-\sum_{k=1}^{K} \frac{|D_{k}|}{|D|} log _{2} \frac{|D_{k}|}{|D|}\)；信息增益：\(Gain(D, a)=H(D)-\sum_{v=1}^{V} \frac{|D_{v}|}{|D|} H(D_{v})\)） | 无法处理连续特征，易过拟合 |
    | 信息增益比（C4.5算法） | 离散+连续 | 分类 | 解决ID3偏向多值特征的问题，更均衡 | 可处理连续特征，支持剪枝 |
    | 基尼系数（CART算法） | 离散+连续 | 分类+回归 | 衡量样本混乱程度，基尼系数越小纯度越高，计算效率高（基尼系数：\(Gini(D)=1-\sum_{k=1}^{K}(\frac{|D_{k}|}{|D|})^{2}\)） | 生成二叉树，应用最广泛 |
  - 剪枝策略（防止过拟合的手段）：
    - 预剪枝：建树时限制条件，提前停止分支。如：设置最大深度max_depth=3，限制树最多3层；设置最小样本数min_samples_leaf=5，叶节点样本少于5则不分支；简单高效但可能欠拟合
    - 后剪枝：先建完整树，再从叶子节点向上剪枝。如：对分支后性能无提升的子树，剪枝为叶节点；效果更好但计算量大
- CART算法构造决策树的基本流程
  1. 特征选择：以基尼指数最小化为目标，遍历所有特征及该特征的所有可能切分点（连续特征需离散化处理），计算每个切分点下数据集的基尼指数，选择基尼指数最小的特征及切分点作为当前节点的“最佳切分属性”与“最佳切分点”
  2. 递归划分：利用“最佳切分属性”和“最佳切分点”，将当前数据集划分为两个子集（如“特征值≤切分点”和“特征值>切分点”），每个子集对应决策树的一个分支
  3. 递归执行：重复步骤1+步骤2，直到满足停止条件之一（子集内所有样本属于同一类别；子集内样本数量小于预设阈值；当前节点的基尼指数小于预设阈值；决策树的深度达到预设上限）
  4. 生成决策树：当所有子集均满足停止条件时，递归过程终止，生成完整的决策树，每个叶节点直接输出多数类作为预测结果
  5. 决策树剪枝（可选）：通过“验证集”评估子树的泛化能力，若子树的验证集精度低于父节点，将子树替换为父节点（即“剪枝”）
- 常见问题与解决办法
  | 常见问题 | 表现 | 解决办法 |
  | --- | --- | --- |
  | 过拟合（树过深） | 训练集准确率100%，测试集准确率低 | 增大min_samples_leaf，限制max_depth，或后剪枝 |
  | 欠拟合（树过浅） | 训练集和测试集准确率均低 | 减小min_samples_split，放宽max_depth限制 |
  | 特征冗余影响效率 | 建树速度慢，决策路径复杂 | 先用特征筛选（如方差过滤）删除无关特征 |

### 2.2 无监督学习
#### 2.2.1 K-Means聚类
- 概念
  - 定义：K-Means是无监督学习中最经典的聚类算法，核心是将数据集自动划分为K个“簇”（Cluster），使同一簇内样本相似度高，不同簇样本相似度低
  - 特点：数据无标签（无需人工标注类别），完全依赖样本自身特征的相似性分组
  - 适用场景：客户分群（如电商用户消费习惯聚类）、文本主题聚类（如新闻按主题分组）、图像分割（如将图片像素按颜色聚类）
  - 相关概念（用“学生按身高分组”类比）：
    | 概念 | 定义 | 类比示例 |
    | --- | --- | --- |
    | 簇（Cluster） | 聚类后形成的组别，同一簇内样本特征相似 | 身高160-170cm的学生组成一组 |
    | K值 | 预设的簇数量（需人工指定） | 计划将学生分为3组（K=3） |
    | 质心（Centroid） | 每个簇的“中心点”（特征平均值） | 某组学生的平均身高165cm，即该组质心 |
    | 距离度量 | 衡量样本与质心的相似性（常用欧氏距离） | 学生身高与组平均身高的差值（差值越小越相似） |
- 原理
  - 思想：将样本划分为K个簇，使簇内样本相似度高、簇间样本相似度低
  - 相似度度量：常用欧氏距离，公式：\(d\left(x_{i}, x_{j}\right)=\sqrt{\sum_{k=1}^{n}\left(x_{i k}-x_{j k}\right)^{2}}\)（\(x_{ik}\)是样本\(x_i\)的第k个特征，\(x_{jk}\)是样本\(x_j\)的第k个特征）；实际计算中常用平方欧氏距离简化运算
  - 步骤：通过“迭代优化”实现聚类，循环执行直到质心稳定（不再变化）
    1. 初始化：选K个初始质心（随机从数据中选择K个样本）；注意：初始质心选择会影响最终结果，算法通常多次运行取最优解（sklearn默认运行10次）
    2. 分配样本：计算每个样本到K个质心的距离，将样本分配到距离最近的质心所在的簇
    3. 更新质心：对每个簇，计算所有样本的特征平均值，作为新的质心
    4. 收敛判断：若聚类中心不再变化或变化小于阈值，停止迭代；否则重复步骤2-3（迭代终止条件：质心变化量小于预设阈值如0.0001，或迭代次数达到上限如100次）
- K-Means聚类中“肘部法”--确定最佳聚类数（k值）的常用技术
  - 原理：通过观察“簇内平方和（SSE）”随k值变化的趋势，找到“聚类效果显著提升与过度细分的平衡点”
  - SSE（Sum of Squared Errors，误差平方和）定义：所有样本到其所属聚类中心的欧氏距离的平方和，是K-Means的目标函数，公式：\(SSE=\sum_{k=1}^{K} \sum_{x_{i} \in C_{k}}\left\|x_{i}-\mu_{k}\right\|^{2}\)（K为聚类数，\(C_k\)是第k个簇，\(x_i\)是簇内样本，\(\mu_k\)是第k个簇的中心）；意义：SSE越小，说明簇内样本越集中（聚类效果越好）
  - 核心思想：随着聚类数k的增加，SSE会持续下降；k较小时，SSE下降速度快；当k超过某个“拐点”后，SSE下降幅度变得极小；这个转折点即为最佳k值
  - 步骤：确定K值范围（如1到10）→计算每个K值的SSE→绘制肘部图（横轴为K值，纵轴为SSE）→识别“肘部”（SSE下降速度显著减缓的拐点）
- 常见问题与解决办法
  | 常见问题 | 表现 | 解决办法 |
  | --- | --- | --- |
  | K值难确定 | 聚类结果不稳定，不知道分几类合适 | 用肘部法（看SSE突变点）或轮廓系数（越大越好）；结合业务场景（如客户分群通常3-5类） |
  | 初始质心敏感 | 多次运行结果不同 | 增加n_init参数（如设为20，多次初始化取最优）；用K-Means++算法（sklearn默认，优化初始质心选择） |
  | 对异常值敏感 | 异常值会拉偏质心 | 聚类前用箱线图或Z-score去除异常值 |
  | 不适用于非凸簇 | 对环形、条形等非凸数据聚类效果差 | 用DBSCAN等密度聚类算法替代 |

#### 2.2.2 主成分分析（PCA，最常用的数据降维算法）
- 概念
  - 目标：在保留数据主要信息（方差）的前提下，降低特征维度，减少计算量；降维原因：高维数据存在“维度灾难”--计算量大、过拟合风险高
  - 定义：PCA（主成分分析）是无监督学习中最经典的降维算法，核心是通过“线性变换”将高维数据映射到低维空间，在损失少量信息的前提下减少特征数量
  - 降维思路：找到数据中“变异最大的方向”（主成分），将数据投影到这些方向上，用更少的维度概括原始数据的主要信息
  - 适用场景：数据可视化（高维→2D/3D）、模型预处理（减少特征数加速训练）、噪声去除（过滤次要信息）
  - 相关概念（用“拍照压缩”类比）：
    | 概念 | 定义 | 类比示例 |
    | --- | --- | --- |
    | 主成分（Principal Component） | 高维数据中“变异最大的方向”（新特征），彼此线性无关 | 照片压缩时保留的“轮廓、主要颜色”等关键信息 |
    | 方差（Variance） | 数据在某一方向上的分散程度，方差越大，该方向包含的信息越多 | 照片中“亮度变化大的区域”（如物体边缘）比“纯色区域”包含更多信息 |
    | 解释方差比（Explained Variance Ratio） | 每个主成分的方差占总方差的比例，衡量该主成分保留的信息比例 | 前2个主成分解释90%方差→降维后保留90%关键信息 |
    | 累计解释方差比 | 前k个主成分的解释方差比之和，用于确定降维后的维度 | 累计解释方差比≥85%→通常认为保留了足够信息 |
- 原理
  - 思想：通过“找主成分→投影降维”两步实现压缩数据，核心是保留“信息最多”的方向
  - 数学原理：
    1. 数据标准化：对每个特征进行标准化（\(x'=\frac{x-\mu}{\sigma}\)），消除量纲影响
    2. 计算协方差矩阵：\(C=\frac{1}{m-1} X^{T} X\)（X为m×n标准化后的特征阵，协方差矩阵C为n×n，\(C_{ij}\)表示第i个特征与第j个特征的协方差）
    3. 求解特征值与特征向量：对协方差矩阵C进行特征分解，得到特征值\(\lambda_{1} ≥\lambda_{2} ≥... ≥\lambda_{n}\)和对应的特征向量\(u_{1}, u_{2}, ..., u_{n}\)（特征向量表示数据方差最大的方向，即主成分方向）
    4. 选择主成分：计算特征值贡献率（\(\frac{\lambda_{i}}{\sum_{j=1}^{n} \lambda_{j}}\)），选择累计贡献率≥85%的前k个特征向量，构成投影矩阵\(U_{k}(n×k)\)
    5. 数据投影：将原始数据X投影到\(U_{k}\)上，得到降维后的数据\(Y=X U_{k}(m×k)\)
  - 步骤：
    1. 找主成分：识别数据的“关键方向”，主成分是数据中方差最大的方向，且每个主成分与前一个主成分正交（避免信息重复）；数学本质：主成分对应原始数据协方差矩阵的特征向量，特征值越大，该主成分的方差越大（包含信息越多）
    2. 投影降维：将高维数据投影到前k个主成分上，得到k维低维数据（k远小于原始维度）
- 常见问题与解决办法
  | 常见问题 | 表现 | 解决办法 |
  | --- | --- | --- |
  | 降维后信息损失过多 | 累计解释方差比低（如<70%），模型性能下降明显 | 增加降维后的维度（如从2维→3维）；若仍无效，改用非线性降维（如t-SNE） |
  | 对量纲敏感 | 特征单位不同（如“年龄（岁）”和“收入（元）”）导致主成分偏向大数值特征 | 降维前必须标准化（用StandardScaler）或归一化 |
  | 主成分无明确业务意义 | 主成分是原始特征的线性组合，难以解释（如“主成分1=0.3×身高+0.7×体重”） | 结合业务知识筛选特征，而非完全依赖PCA；或接受“牺牲可解释性换效率” |

#### 2.2.3 关联规则挖掘（Apriori算法，电商推荐技术）
- 概念
  - 定义：关联规则挖掘是无监督学习中发现数据项之间关联关系的技术，核心是从大量交易数据中找出“项集之间的频繁关联”（如尿布→啤酒）
  - 特点：无需标签，直接从交易数据中挖掘隐藏规律（如顾客的购买习惯）
  - 适用场景：超市商品推荐、电商“买了又买”功能、网页点击路径分析、医疗疾病并发症关联分析
  - 关联规则的要素（以规则A→B为例）：
    - 项集（Itemset）：交易中出现的物品集合（单个项集：尿布、啤酒；组合项集：尿布，啤酒）
    - 支持度（Support）：项集在所有交易中出现的概率（衡量项集的普遍性），公式：\(Support (A \to B)=\frac{包含A和B的交易数}{总交易数}\)
    - 置信度（Confidence）：买了A之后又买B的概率（衡量规则的可靠性），公式：\(Confidence (A \to B)=\frac{包含A和B的交易数}{包含A的交易数}\)
    - 提升度（Lift）：买A后对买B的促进作用（排除随机巧合），公式：\(Lift(A \to B)=\frac{Confidence (A\to B)}{Support(B)}\)
  - 阈值：
    - 支持度阈值（如≥5%）：过滤掉太罕见的项集
    - 置信度阈值（如≥50%）：确保规则有足够可靠性
    - 提升度>1：说明A对B有促进作用（提升度=1表示无关，<1表示抑制）
- 原理
  - 思想：Apriori算法解决了“暴力枚举所有项集”的效率问题，核心是“频繁项集生成→关联规则提取”
    - 频繁项集：支持度≥最小支持度的项集
    - 关联规则：形如“A→B”的规则（A为前件，B为后件，A∩B=∅），需满足支持度和置信度指标
  - 步骤：
    1. 生成频繁项集：
      - 关键剪枝逻辑（Apriori原理）：“非频繁项集的超集一定是非频繁的”
      - 迭代过程：①找频繁1-项集（单个物品）；②用频繁1-项集组合生成2-项集，过滤掉非频繁的；③以此类推，直到无法生成更高阶的频繁项集
    2. 从频繁项集提取关联规则：对每个频繁项集，拆分出所有可能的规则，计算每个规则的置信度和提升度，保留满足阈值的规则（如置信度≥50%、提升度>1）
- 常见问题与解决办法
  | 常见问题 | 表现 | 解决办法 |
  | --- | --- | --- |
  | 规则太多无意义 | 输出成百上千条规则，多数无业务价值 | 提高支持度/置信度阈值；优先看提升度高的规则（如lift>2）；结合业务场景筛选（如只关注高利润商品） |
  | 计算速度慢 | 物品数量多（如1000种商品）时，算法运行几小时 | 减少物品数量（过滤低频商品）；降低项集最大长度（如只找≤3个物品的规则）；用FP-Growth算法替代Apriori（更高效） |
  | 虚假关联 | 规则提升度>1但无实际关联（如雨伞→冰淇淋，实际因季节相关） | 结合领域知识验证；增加时间/空间维度分析（如分季节挖掘） |

### 2.3 半监督学习
- 核心思想：由于标注成本高，标注数据很少，无标签数据丰富。半监督学习就是让标签数据“带动”无标签数据，共同学习分类规则
- 算法目标：用少量标签数据（10%~30%）+大量无标签数据，达到接近全标签数据的分类精度，降低标注成本

#### 2.3.1 标签传播（Label Propagation, LP）--“学生互助，标签扩散”
- 思想：
  - 类比：班级里少数学生知道答案（标签数据），大家通过“互相讨论”（相似度连接），让答案逐渐扩散到全班（无标签数据），最终形成统一的“解题思路”（分类结果）
  - 定义：基于“图模型”的半监督学习算法，将所有样本视为图节点（标签样本+无标签样本），用边权重表示样本相似度，通过“标签在图上的传播迭代”，让无标签样本的标签逐渐收敛到稳定值，实现分类
- 逻辑与步骤
  - 算法逻辑：
    1. 构建样本图：每个样本是节点，边权重=样本相似度（相似度越高，权重越大）
    2. 初始化标签：标签样本保留真实标签，无标签样本设为初始值（如全0或随机）
    3. 标签传播：每个无标签样本的标签=所有样本的标签×对应边权重（加权平均），迭代更新
    4. 收敛终止：当标签变化量小于阈值，或达到最大迭代次数，停止传播，输出最终标签
  - 步骤：
    1. 相似度计算（边权重）：
      - 高斯核相似度（适用于连续数据）：\(w_{ij}=\exp (-\frac{\left\|x_{i}-x_{j}\right\|^{2}}{2 \sigma^{2}})\)（σ为核宽度，控制相似度衰减速度）
      - 余弦相似度（适用于高维数据如文本）：\(w_{ij}=\frac{x_{i} \cdot x_{j}}{\left\|x_{i}\right\| \cdot\left\|x_{j}\right\|}\)
    2. 标签传播公式（迭代更新）：
      - 归一化权重矩阵T（每行求和为1）：\(T_{ij}=\frac{w_{ij}}{\sum_{k} w_{ik}}\)
      - 标签更新：\(Y^{t+1}=T × Y^{t}\)（Y为标签矩阵，t为迭代次数）
      - 核心：标签按相似度加权扩散，相似样本的标签相互影响
    3. 收敛条件：\(\left\|Y^{t+1}-Y^{t}\right\|<\epsilon\)（ε为极小值，如1e-5）
- 主要参数与效果影响
  | 参数 | 含义 | 影响 |
  | --- | --- | --- |
  | sigma（高斯核宽度） | 控制样本相似度的衰减速度（sklearn中用gamma，与sigma成反比：gamma=1/(2σ²)） | σ越大，相似度衰减越慢，标签传播范围越广；σ越小，仅邻近样本传播标签 |
  | max_iter | 最大迭代次数 | 次数过少可能未收敛，次数过多浪费算力，默认100次足够多数场景 |
  | tol | 收敛阈值 | 阈值越小，收敛越彻底，但迭代次数增加，默认1e-5即可 |
- 优缺点与适用场景
  | 优点 | 缺点 | 适用场景 |
  | --- | --- | --- |
  | 无参数学习（仅需调相似度相关参数） | 对噪声数据敏感（异常样本会误导标签传播） | 低维数据、聚类效果明显的数据（如图像、简单文本） |
  | 迭代过程简单，计算效率高 | 对相似度参数（σ/gamma）敏感，需调优 | 标注成本高、样本分布密集的场景 |
  | 可处理多分类问题 | 不适用于高维稀疏数据（如高维文本） | 小样本半监督分类（标签比例10%~30%） |

## 3 强化学习
### 3.1 强化学习基础概念
#### 3.1.1 定义
- 强化学习（Reinforcement Learning, RL）：智能体（Agent）通过与环境（Environment）持续交互，根据环境反馈的奖励（Reward）调整行为，逐步学习到“最大化累计奖励”的最优策略（Policy）的学习范式
- 特点：
  - 无标注数据（靠环境反馈学习）
  - 序列决策（动作影响后续状态和奖励）
  - 试错学习（允许初期犯错，从错误中优化）
- 强化学习与监督/无监督学习的区别
  | 学习类型 | 数据特点 | 核心逻辑 | 典型场景 |
  | --- | --- | --- | --- |
  | 监督学习 | 有标注数据（输入→标签） | 学习“输入到标签的映射” | 图像分类（输入图片→输出类别标签） |
  | 无监督学习 | 无标注数据 | 学习“数据的内在结构” | 客户分群（从消费数据中找相似客户组） |
  | 强化学习 | 无标注数据，有环境奖励 | 学习“状态到动作的最优策略” | 机器人导航、游戏AI、自动驾驶 |

#### 3.1.2 强化学习的要素
以“迷宫寻宝”场景（智能体找宝藏，避开陷阱）为例，对应六大要素：
| 要素 | 定义 | 迷宫寻宝场景示例 |
| --- | --- | --- |
| 智能体(Agent) | 做出决策、执行动作、学习策略的主体 | 迷宫中寻找宝藏的“小机器人” |
| 环境(Environment) | 智能体所处的外部场景，会对智能体的动作做出反馈 | 包含宝藏、陷阱、路径的“迷宫地图” |
| 状态(State, S) | 智能体在环境中的当前处境（描述环境和智能体的当前情况） | 小机器人的位置坐标（如(2,3)）、迷宫中已探索的区域 |
| 动作(Action, A) | 智能体在当前状态下可执行的操作（动作集合固定或动态） | 小机器人的4个方向：上、下、左、右（离散动作） |
| 奖励(Reward, R) | 环境对智能体动作的即时反馈（正奖励鼓励，负奖励惩罚） | 找到宝藏:+100分；掉入陷阱:-50分；走普通路径:-1分（鼓励高效寻宝） |
| 策略(Policy, π) | 智能体从“状态→动作”的映射规则（决定在什么状态下做什么动作） | 策略1：优先向右走；策略2：避开已探索的陷阱区域，向未探索区域移动 |

#### 3.1.3 强化学习的目标与任务类型
- 目标：学习最优策略π*（策略：状态到动作的映射，如\(\pi(s)=a\)表示在状态s下执行动作a），使智能体在长期交互中获得的累积奖励最大化
- 强化学习的目标不是“单次动作奖励最大”，而是“长期累计奖励最大”（如迷宫中，为了最终+100分，愿意接受多次-1分的普通路径，避免-50分的陷阱）
- 累积奖励计算：
  - 有限步数：\(G_{t}=R_{t+1}+R_{t+2}+...+R_{T}\)（T为终止时刻）
  - 无限步数（避免奖励无穷大）：\(G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+...\)（γ为折扣因子，0<γ≤1，γ越接近1越重视未来奖励）
- 任务类型：
  - episodic任务（回合制）：有明确的开始和结束（如“一局游戏”“一次机器人导航”）
  - continuous任务（持续制）：无明确结束，智能体持续与环境交互（如“自动驾驶汽车在道路上行驶”）

#### 3.1.4 强化学习的步骤
以“迷宫寻宝”为例，说明强化学习从试错到最优的学习过程：
1. 交互--智能体执行动作：智能体在当前状态St（如位置(2,3)），根据当前策略π（如优先向右），选择动作At（向右走）；环境接收动作At，切换到新状态St+1（如位置(2,4)），并反馈即时奖励Rt+1（如-1分，普通路径）
2. 学习--更新策略：智能体根据“动作At→新状态St+1→奖励Rt+1”的反馈，判断当前动作是否“划算”；若动作带来正奖励（如靠近宝藏，+5分），则强化该策略（未来在St状态下更可能选At）；若带来负奖励（如靠近陷阱，-10分），则弱化该策略（未来在St状态下避免选At）
3. 迭代--循环优化：重复“交互→学习”过程，智能体不断调整策略，逐步减少错误动作（如避开陷阱），增加有效动作（如向宝藏方向移动）；最终收敛到“最优策略”：在任意状态下，都能选择让“长期累计奖励最大”的动作（如直接避开所有陷阱，最短路径找到宝藏）

#### 3.1.5 强化学习的应用场景
- 游戏AI：AlphaGo（围棋）、DOTA2 AI（复杂团队对战），通过强化学习超越人类顶尖选手
- 机器人导航：仓储机器人避开障碍物、自主规划路径，适应动态变化的仓库环境
- 自动驾驶：车辆在复杂路况（如雨天、拥堵）中，学习“加速/减速/变道”的最优策略
- 资源调度：电网负荷调度、物流路径优化，通过强化学习最大化资源利用效率

### 3.2 马尔可夫决策过程（MDP）：强化学习的数学模型
#### 3.2.1 定义
- 马尔可夫性：未来的状态只取决于当前状态和动作，与过去的状态无关。如迷宫中，智能体下一步的位置只和现在的位置、选的方向有关，和之前走了哪条路无关
- 意义：简化问题复杂度，无需记忆历史状态，仅需当前状态即可决策
- 马尔可夫决策过程（MDP）：用“状态、动作、随机转移、奖励”，结合马尔可夫性，对强化学习环境进行数学建模的框架，是绝大多数强化学习算法（如Q-Learning、PPO）的理论基础

#### 3.2.2 MDP的五要素（S, A, P, R, γ）
MDP的数学表示：MDP可简化表示为五元组：\(M=(S, A, P, R, \gamma)\)，其中：
- S和A定义“决策的范围”
- P定义“环境的随机性”
- R定义“学习的目标导向”
- γ定义“短期与长期奖励的平衡”

以“迷宫寻宝”场景为例（新增随机性：10%概率打滑，动作方向偏移），说明MDP的五要素：
| 要素 | 数学符号 | 定义 | 迷宫寻宝场景示例 |
| --- | --- | --- | --- |
| 状态空间(State Space) | S | 所有可能状态的集合（离散或连续） | 迷宫中所有位置坐标的集合：S={(0,0),(0,1),...,(3,3)}（共16个状态） |
| 动作空间(Action Space) | A | 所有可能动作的集合（每个状态下的可用动作可能不同） | 智能体的4个方向：A={上,下,左,右}（离散动作空间） |
| 转移概率(Transition Probability) | P(s'|s,a) | 当智能体处于状态s并选择动作a时，转移到状态s'的概率 | 在状态s执行动作a后，转移到状态s'的概率（MDP的随机性核心） |
| 奖励函数(Reward Function) | R(s,a,s')或R(s,a) | 状态从s经动作a转移到s'时获得的即时奖励（可依赖状态或动作） | 从(0,0)经“右”到(0,1)（普通路径）：R=-1；从(3,2)经“右”到(3,3)（宝藏）：R=100 |
| 折扣因子(Discount Factor) | γ | 0<γ<1，衡量未来奖励的重要程度（γ越大，越重视未来奖励） | 取γ=0.9：未来1步的+100奖励，相当于当前的0.9×100=90；未来2步的+100奖励，相当于当前的0.9²×100=81 |

#### 3.2.3 MDP的“状态转移与奖励流程”
以“迷宫中从状态s=(0,0)执行动作a=右”为例，解释MDP的决策循环：
1. 智能体选择动作：智能体处于当前状态s∈S（如(0,0)），根据策略π选择动作a∈A（如“右”）
2. 环境随机转移状态：环境根据转移概率P(s'|s,a)，随机生成新状态s'∈S（如90%到(0,1)，10%到(1,0)）--这是MDP与“确定性环境”的核心区别（引入随机性）
3. 环境反馈即时奖励：智能体获得从s经a到s'的即时奖励R(s,a,s')（如到(0,1)得-1，到(1,0)得-1），并进入新状态s'，开始下一轮决策循环

#### 3.2.4 MDP在强化学习中的作用
- 环境建模：将复杂的现实环境（如自动驾驶、机器人导航）转化为“状态-动作-奖励”的数学模型，让算法可计算
- 策略优化：基于MDP的转移概率和奖励函数，可推导“最大化累计奖励”的最优策略
- 算法统一：Q-Learning、SARSA、PPO等算法，本质都是“在MDP框架下求解最优策略”的不同实现

#### 3.2.5 常见问题与扩展
| 常见问题 | 表现 | 解决思路/扩展模型 |
| --- | --- | --- |
| 状态空间过大（维数灾难） | 连续状态（如图像像素）或高维离散状态，无法枚举所有s | 用函数近似（如神经网络）表示价值/策略；用POMDP（部分可观测MDP）处理不可见状态 |
| 转移概率未知 | 现实环境中难以精准测量P(s’|s,a)（如自动驾驶的路况概率） | 直接通过智能体与环境交互采样经验(s,a,r,s')，学习价值函数或最优策略；或用贝叶斯强化学习（BRL）对未知转移概率建模为概率分布，通过交互数据更新分布并优化策略 |
| 奖励设计困难 | 奖励稀疏或不合理导致策略无法收敛（如机器人导航只有终点有奖励） | 设计shaped reward（中间奖励）；用逆强化学习（IRL）从专家行为反推奖励函数 |

### 3.3 强化学习算法：Q学习（基于价值的无模型算法）
#### 3.3.1 定义
Q学习（Q-Learning）：一种基于“动作价值函数（Q函数）”的无模型强化学习算法，通过构建Q表记录每个“状态-动作”对的价值Q(s,a)，利用环境反馈的奖励，迭代更新Q值，最终让智能体学会“在每个状态下选择Q值最大的动作”，实现最大化累计奖励

#### 3.3.2 Q学习的要素
以“迷宫寻宝”场景（智能体找宝藏，避开陷阱，环境转移概率未知）为例，说明Q学习的要素：
| 要素 | 定义 | 数学表示/场景示例 |
| --- | --- | --- |
| Q表（Q-Table） | 记录所有“状态-动作”对价值的表格，Q(s,a)表示“在状态s执行动作a的长期累计奖励期望” | 迷宫场景：行=状态（如(0,0)），列=动作（上、下、左、右），单元格值=Q(s,a)（初始为0） |
| 学习率（Learning Rate, α） | 0<α<1，控制每次Q值更新的幅度（α越大，新反馈对Q值影响越大） | α=0.1：Q新=Q旧+0.1×(目标Q - Q旧)（缓慢更新，更稳定） |
| 探索率（Exploration Rate, ε） | 0<ε<1，控制智能体“探索新动作”与“利用已知最优动作”的平衡（ε-贪心策略） | ε=0.1：90%概率选当前Q值最大的动作（利用），10%概率随机选动作（探索） |
| 贝尔曼更新方程（Bellman Update） | Q学习的核心，用“即时奖励+未来最优Q值”更新当前Q值 | 公式：\(Q(s, a) \leftarrow Q(s, a)+\alpha\left[R+\gamma max _{a'} Q\left(s', a'\



x_new = x_i + λ(x_j − x_i)
