# ## **1. 引言：vLLM 推理引擎定位**

### 📌 核心思想

vLLM 是一个专门用于大语言模型推理（Inference）的高性能引擎，目标是：

* **让 LLM 跑得更快**
* **让 GPU 充分利用（减少空转）**
* **降低成本（更少 GPU 做更多请求）**

### 📌 为什么需要 vLLM？

普通推理框架的问题：

* 请求分散 → GPU 空闲时间多
* 一个用户请求速度快，但多个用户时吞吐下降
* KV 缓存占用大量内存

vLLM 的使命：**用更聪明的方式调度、缓存、并行计算，实现高吞吐低延迟**。

---

# ## **2. 核心优化技术**

## 🟦 2.1 “引擎循环优化”（Engine Loop Optimization）

推理过程核心是一个不断执行的循环：

```
收集请求 → 组批(batch) → 运行模型 → 输出 token → 下一轮
```

问题在于：

* 每轮循环都要做调度 → 有开销
* 批次大小不稳定 → GPU 利用率低

### 🔧 vLLM 的优化方式：

### **（1）输入并行化 Input Parallelism**

让不同用户的输入在同一批次中一起处理。

#### 📘 示例

* 用户 A 输入 20 tokens
* 用户 B 输入 50 tokens
* 用户 C 输入 10 tokens

普通框架：分别处理（GPU 空转很多）
vLLM：合并成一个 batch 处理（大幅提高吞吐）

---

### **（2）分段 CUDA 图（Segmented CUDA Graph）**

CUDA 图 = 把 GPU 操作录制成“固定流程”，减少 Python → GPU 调用耗时。

但 LLM 的 batch 大小是动态的，不可能录一个固定图。

vLLM 的解决方法：

#### 📌 “分段”

* 对始终一致的部分录制 CUDA 图
* 对可变部分允许动态执行

这样既能提速，又能保持灵活。

---

## 🟦 2.2 重点指标优化（TTFT & TPOT）

* **TTFT（Time To First Token）**：从用户发起请求到第一个 token 输出
* **TPOT（Tokens Per Output Time）**：模型生成 token 的速度（吞吐）

vLLM 的优化让：

* **TTFT 更短**（感觉更快）
* **TPOT 更高**（服务器处理更多用户）

---

# ## **3. 推理加速策略**

## 🟩 3.1 级联推理（Cascade Inference）

核心：**多个请求的前缀通常相同，可以复用计算**。

### 📘 示例（典型场景：聊天机器人）

用户常常输入：

```
用户：你好，请帮我写一篇关于 AI 的文章
用户：你好，请帮我写一篇关于教育的文章
用户：你好，请帮我写一篇关于旅游的文章
```

前缀“你好，请帮我写一篇关于”完全相同。

普通推理：

* 每个请求从头算一次

vLLM 级联推理：

* 前缀只算一次，三个用户复用
* GPU 内存访问更高效
* 提升吞吐最多可达 **几十倍**

---

## 🟩 3.2 推测性解码（Speculative Decoding）

核心：
**小模型预测，大模型验证** → 大大减少重复计算。

### 📘 示例

假设：

* 大模型：70B
* 小模型：7B（快10倍）

过程：

1. **小模型先预测**未来可能的 5 个 token
2. **大模型检查预测**是否正确
3. 正确的部分直接采用，不用让大模型重新算
4. 小模型继续预测下一段…

效果：
大模型的计算量大幅减少 → **推理速度提升可达到 2–4 倍**

---

## 🟩 3.3 KV 缓存（KV Cache）

Transformer 中 Self-Attention 会生成：

* K（Key）
* V（Value）

这些随 token 累积：

```
第1轮 token → 保存 K1, V1
第2轮 token → 保存 K2, V2
……
```

### 📘 举例

如果生成了 1000 个 token，那么模型在第 1001 个 token 计算时需要注意：

```
[K1…K1000], [V1…V1000]
```

**缓存的意义**：避免每次重复计算所有历史 token 的 KV。

vLLM 的 KV 管理特别高效：

* 动态分配
* 支持共享
* 支持前缀复用

显著减少显存，使 **一次 GPU 能服务更多用户**。

---

# ## 4. 并行与分布式推理

当模型大到无法放到单卡 → 必须拆分。

### vLLM 支持 4 种并行方式：

---

## 🟨 4.1 张量并行（Tensor Parallel, TP）

把 **单个层内部的矩阵计算拆分到多个 GPU 上**。

### 📘 示例

一个大矩阵乘法：

```
[W1 W2]
```

拆到 2 张卡执行，然后合并。

适用于：大参数但每层结构统一。

---

## 🟨 4.2 流水线并行（Pipeline Parallel, PP）

把模型的层按顺序拆到不同 GPU 上。

### 📘 示例

* GPU0：处理 0–10 层
* GPU1：处理 11–20 层
* GPU2：处理 21–30 层

前一个 GPU 输出就是下一个 GPU 输入。

适用于：层数多的大模型。

---

## 🟨 4.3 专家并行（MoE Parallel）

MoE 模型中存在多个“专家”，每个 GPU 保存一部分专家。

### 📘 类比

像医院里不同科室（专家）各自处理自己的部分。

---

## 🟨 4.4 数据并行（DP）

每个 GPU 保持一份全模型副本，处理不同 batch 的数据。

适用于：简单的多用户处理。

---

## 🟨 混合并行（Hybrid Parallel）

实际部署中通常 **TP + PP + DP + MoE** 一起用。

---

# ## 5. 量化技术（Quantization）

核心思想：
**用低精度数字表示模型参数，减少显存 + 提升推理速度。**

常见量化策略：

* FP16 → FP8
* FP16 → INT8
* 高级版本如 GPTQ、AWQ 用于离线量化

### 📘 举例

假设原模型 70B：

| 精度   | 模型显存     |
| ---- | -------- |
| FP16 | 140 GB   |
| INT8 | 70 GB    |
| FP8  | 40–50 GB |

这意味着：

→ 同一台服务器能跑更多模型
→ 推理速度更快（访存更快）
→ 成本更低

vLLM 内置量化支持，因此使用非常方便。

---

# ## 6. 使用场景与价值

vLLM 特别适合：

### 🟩 **（1）在线服务（API 服务）**

例如：

* 在线对话型 AI（ChatGPT 类应用）
* 写作助理
* 智能客服

它让服务可以同时处理更多用户。

---

### 🟩 **（2）长上下文场景**

vLLM 的 KV 管理很强，使：

* 128k context
* 256k context
* 甚至百万上下文扩展

都可以更高效地运行。

---

### 🟩 **（3）多用户共享 GPU**

因为 vLLM 通过动态调度与共享 KV，使很多用户共享 GPU 很高效。

传统模型负载大时会卡顿，vLLM 不会。

---


